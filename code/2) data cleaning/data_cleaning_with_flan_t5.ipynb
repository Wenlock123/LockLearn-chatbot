{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**วิธีเเรกใช้ google/flan-t5-base ช่วย clean**"
      ],
      "metadata": {
        "id": "UuT63hYp3a9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ไม่เเนะนำวิธีเนื่องจากเป็นโมเดลที่ออกมาค่อนข้างนานเเล้ว เเต่ที่ลอง clean ด้วยวิธีนี้จะได้เปรียบเทียบได้ง่ายๆ กับอีกวิธี"
      ],
      "metadata": {
        "id": "myyU7tvI4Hhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "เชื่อม google drive"
      ],
      "metadata": {
        "id": "Xgdgg1oo4V5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "9t_qZbKG4ZNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIpcDUBA3RpA"
      },
      "outputs": [],
      "source": [
        "# ✅ ติดตั้งไลบรารีที่จำเป็น\n",
        "!pip install transformers accelerate sentencepiece --quiet\n",
        "!pip install pytesseract pdf2image transformers accelerate \\\n",
        "    sentence-transformers faiss-cpu langchain \\\n",
        "    unstructured chromadb streamlit \\\n",
        "    torchvision torchaudio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# โหลดโมเดล (ไม่ต้องใช้ HF Token)\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=False)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=False).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "ZsDSNhEZ3gX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "โหลดข้อมูลจาก google drive ที่เราทำการรวมเข้าด้วยกัน"
      ],
      "metadata": {
        "id": "tOLNY-pz3ig-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# โหลดข้อความจาก Google Drive\n",
        "file_path = \"/content/drive/MyDrive/LockLearn/combined_text.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n"
      ],
      "metadata": {
        "id": "BPj4KYai3iKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# แบ่งข้อความออกเป็นหน้า ๆ (เช่น ทุก ๆ 40 บรรทัด = 1 หน้า)\n",
        "# ลอง\n",
        "lines = raw_text.split(\"\\n\")\n",
        "pages = [\"\\n\".join(lines[i:i+40]) for i in range(0, len(lines), 40)]\n"
      ],
      "metadata": {
        "id": "sgxYuNfY3qDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ฟังก์ชันให้ LLM ช่วย clean หน้า\n",
        "def clean_text_with_llm(text):\n",
        "    prompt = f\"Please correct any spacing errors, fix sentence breaks, and clean up OCR mistakes in the following text:\\n{text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "6_aX3_Nh3sQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ประมวลผลทีละหน้า\n",
        "cleaned_pages = []\n",
        "for i, page in enumerate(pages):\n",
        "    print(f\"✨ Cleaning page {i+1}/{len(pages)}...\")\n",
        "    cleaned = clean_text_with_llm(page)\n",
        "    cleaned_pages.append(cleaned)\n"
      ],
      "metadata": {
        "id": "vcoR9miI3taY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "รวมทุกหน้าเข้าด้วยกัน"
      ],
      "metadata": {
        "id": "qIIQRQpn3vn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# รวมทุกหน้ากลับเป็นไฟล์เดียว\n",
        "output_path = \"/content/drive/MyDrive/LockLearn/cleaned_text.txt\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\\n--- PAGE BREAK ---\\n\\n\".join(cleaned_pages))\n",
        "\n",
        "print(\"✅ เสร็จแล้ว! ไฟล์ถูกบันทึกที่:\", output_path)"
      ],
      "metadata": {
        "id": "Oig56bhV3vJW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}